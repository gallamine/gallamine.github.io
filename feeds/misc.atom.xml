<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>William's Blog</title><link href="http://gallamine.com/" rel="alternate"></link><link href="http://gallamine.com/feeds/misc.atom.xml" rel="self"></link><id>http://gallamine.com/</id><updated>2013-01-21T20:51:00+01:00</updated><entry><title>A Sigmoid Function Without the Exponential: Elliot Sigmoid Activation Function</title><link href="http://gallamine.com/a-sigmoid-function-without-the-exponential-elliot-sigmoid-activation-function.html" rel="alternate"></link><updated>2013-01-21T20:51:00+01:00</updated><author><name>William 'Gallamine' Cox</name></author><id>tag:gallamine.com,2013-01-21:a-sigmoid-function-without-the-exponential-elliot-sigmoid-activation-function.html</id><summary type="html">&lt;p&gt;&lt;img alt="Sigmoid Function" src="/images/2013-01-21-a-sigmoid-function-with-out-the-exponential-elliot-sigmoid-activation-function_ElliotVsExp.png" /&gt;&lt;/p&gt;
&lt;p&gt;Today I learned about the Elliot Activation (or Sigmoid) Function. In fact, The MathWorks just included it in their &lt;a href="http://www.mathworks.com/help/nnet/ref/elliotsig.html"&gt;most recent update to the Neural Network toolbox&lt;/a&gt;. The function was first introduced in 1993 by D.L. Elliot under the title &lt;a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.46.7204"&gt;A Better Activation Function for Artificial
Neural Networks&lt;/a&gt;. The function closely approximates the Sigmoid or Hyperbolic Tangent functions for small values, however it takes longer to converge for large values (i.e. It doesn't go to 1 or 0 as fast), though this isn't particularly a problem if you're using it for classification. &lt;/p&gt;
&lt;p&gt;In my testing in MATLAB "the function computes over &lt;strong&gt;2x faster&lt;/strong&gt; than the exponential sigmoid function", which, for certain types of ML problems can lead to a significant speed improvement. So what is this function? &lt;/p&gt;
&lt;p&gt;Behold:&lt;/p&gt;
&lt;p&gt;\begin{aligned}
\sigma_e(x) = \frac{1}{1 + |x|}
\end{aligned}&lt;/p&gt;
&lt;p&gt;It's also differntiable with the derivative of &lt;/p&gt;
&lt;p&gt;\begin{aligned}
\frac{\partial\sigma_e(x)}{\partial x} = \frac{1}{(1 + |x|)^2}
\end{aligned}&lt;/p&gt;
&lt;p&gt;For an activation function in the range $ [0,1] $ &lt;a href="http://www.heatonresearch.com/wiki/Elliott_Activation_Function"&gt;it can be written as&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;\begin{aligned}
\sigma_e(x) = \frac{0.5(x)}{1 + |x|} + 0.5
\end{aligned}&lt;/p&gt;
&lt;p&gt;In MATLAB this can be simply written as&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;function&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;p &lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;elliotsig&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;x&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="c"&gt;% From http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.46.7204&lt;/span&gt;
&lt;span class="c"&gt;% &amp;quot;A better Activation Function for Artificial Neural Networks&amp;quot;&lt;/span&gt;

&lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nb"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;    

&lt;span class="k"&gt;end&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Here's an image I ganked from Wikipedia that shows the Elliot, Exponential and Hyperbolic Tangent Sigmoid functions:&lt;/p&gt;
&lt;p&gt;&lt;a href="http://en.wikipedia.org/wiki/File:Gjl-t(x).svg"&gt;&lt;img alt="Sigmoid Function" src="/images/2013-01-21-a-sigmoid-function-with-out-the-exponential-elliot-sigmoid-activation-function_sigmoid.png" /&gt;&lt;/a&gt;&lt;/p&gt;</summary></entry><entry><title>"NFL Bad Lipreading"</title><link href="http://gallamine.com/nfl-bad-lipreading.html" rel="alternate"></link><updated>2013-01-19T07:09:00+01:00</updated><author><name>William 'Gallamine' Cox</name></author><id>tag:gallamine.com,2013-01-19:nfl-bad-lipreading.html</id><summary type="html">&lt;iframe width="560" height="315" src="http://www.youtube.com/embed/Zce-QT7MGSE" frameborder="0" allowfullscreen&gt;&lt;/iframe&gt;</summary></entry><entry><title>Binary Search Algorithm in MATLAB</title><link href="http://gallamine.com/binary-search-algorithm-in-matlab.html" rel="alternate"></link><updated>2011-01-19T06:40:00+01:00</updated><author><name>William 'Gallamine' Cox</name></author><id>tag:gallamine.com,2011-01-19:binary-search-algorithm-in-matlab.html</id><summary type="html">&lt;p&gt;Here’s how I did a quick binary search in MATLAB:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;    &lt;span class="n"&gt;randNum&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;rand&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;

    &lt;span class="n"&gt;minIndex&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

    &lt;span class="n"&gt;maxIndex&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;length&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cdf_scatterSM&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;

    &lt;span class="n"&gt;midIndex&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="n"&gt;minIndex&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nb"&gt;round&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;maxIndex&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;minIndex&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;

    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="n"&gt;randNum&lt;/span&gt; &lt;span class="o"&gt;~=&lt;/span&gt; &lt;span class="n"&gt;cdf_scatterSM&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;midIndex&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;maxIndex&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="n"&gt;minIndex&lt;/span&gt;

        &lt;span class="n"&gt;midIndex&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="n"&gt;minIndex&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nb"&gt;round&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;maxIndex&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;minIndex&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;

        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;randNum&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;cdf_scatterSM&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;midIndex&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="n"&gt;minIndex&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="n"&gt;midIndex&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

        &lt;span class="k"&gt;elseif&lt;/span&gt; &lt;span class="n"&gt;randNum&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;cdf_scatterSM&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;midIndex&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="n"&gt;maxIndex&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="n"&gt;midIndex&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

        &lt;span class="k"&gt;end&lt;/span&gt;

    &lt;span class="k"&gt;end&lt;/span&gt;

    &lt;span class="n"&gt;midIndex&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="n"&gt;minIndex&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nb"&gt;round&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;maxIndex&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;minIndex&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;

    &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="n"&gt;midIndex&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Note: The index was then used for linear interpolation, hence the ‘minIndex’ value starting at 2 instead of 1 (remember, MATLAB indexes from 1).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Edit 5/31/11:&lt;/strong&gt; I just discovered a bug in this algorithm (I changed it above). The while loop 2nd condition should be “maxIndex &amp;gt;= minIndex”. Apologies.&lt;/p&gt;</summary></entry></feed>